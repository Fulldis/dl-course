{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seminar_done.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwRfuC6RDb1B"
      },
      "source": [
        "# Seminar 2. PyTorch\n",
        "Hi! Today we are going to study PyTorch. We'll compare numpy and PyTorch commands, rewrite our previous neural network in two ways.\n",
        "\n",
        "!!! GPU ON !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilG0uJwVcke4"
      },
      "source": [
        "! pip install numpy scikit-learn matplotlib seaborn mnist torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMKPD9TZdokn"
      },
      "source": [
        "from IPython import display\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gsxpRimFHF-"
      },
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7xxXYAeFofM"
      },
      "source": [
        "## Numpy vs Pytorch\n",
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbIPHFs6FmkB"
      },
      "source": [
        "a = [1. , 1.4 , 2.5]\n",
        "print(f\"Simple way: {torch.tensor(a)}\")\n",
        "print(f\"Zeros:\\n {torch.zeros((2,3))}\")\n",
        "print(f\"Range: {torch.arange(0, 10)}\")\n",
        "print(f\"Complicated range: {torch.arange(4, 12, 2)}\")\n",
        "print(f\"Space: {torch.linspace(1, 4, 6)}\")\n",
        "print(f\"Identity matrix:\\n {torch.eye(4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWKCkXaSGK1U"
      },
      "source": [
        "### Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHFgnpzOF5yb"
      },
      "source": [
        "print(f\"From 0 to 1: {torch.rand(1)}\")\n",
        "print(f\"Vector from 0 to 1: {torch.rand(5)}\")\n",
        "print(f\"Vector from 0 to 10: {torch.randint(10, size=(5,))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EmP3g0wGfzT"
      },
      "source": [
        "### Matrix Operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfUdENnEF8-l"
      },
      "source": [
        "a = torch.arange(10).type(torch.FloatTensor)\n",
        "b = torch.linspace(-10, 10, 10)\n",
        "print(f\"a: {a}\\nshape: {a.size()}\")\n",
        "print(f\"b: {a}\\nshape: {b.size()}\")\n",
        "print(f\"a + b: {a + b},\\n a * b: {a * b}\")\n",
        "print(f\"Dot product: {a.dot(b)}\")\n",
        "print(f\"Mean: {a.mean()}, STD: {a.std()}\")\n",
        "print(f\"Sum: {a.sum()}, Min: {a.min()}, Max: {a.max()}\")\n",
        "print(f\"Reshape:\\n{a.reshape(-1, 1)}\\nshape: {a.reshape(-1, 1).size()}\")\n",
        "c = a.reshape(-1, 1).repeat(1, 5)\n",
        "print(f\"Repeat:\\n{c}\\nshape: {c.size()}\")\n",
        "print(f\"Transpose:\\n{c.T}\\nshape: {c.T.size()}\")\n",
        "print(f\"Unique items: {torch.unique(c)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdhrhXj9H0m6"
      },
      "source": [
        "### Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqXM0bQGrRv"
      },
      "source": [
        "a = torch.arange(100).reshape(10, 10)\n",
        "print(f\"Array:\\n{a}\\nshape: {a.size()}\")\n",
        "print(f\"Get first column: {a[:, 0]}\")\n",
        "print(f\"Get last row: {a[-1, :]}\")\n",
        "print(f\"Add new awis:\\n{a[:, np.newaxis]}\\nshape: {a[:, np.newaxis].size()}\")\n",
        "print(f\"Specific indexing:\\n{a[4:6, 7:]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFQ0OwEkIHu8"
      },
      "source": [
        "### Numpy <-> Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oueWvyMzH6sZ"
      },
      "source": [
        "a = torch.normal(mean=torch.zeros(2,4))\n",
        "a.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRCr6c78QeH1"
      },
      "source": [
        "b = np.random.normal(size=(2, 4))\n",
        "torch.from_numpy(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJyktewrROPL"
      },
      "source": [
        "### CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JshOGbX0RAb1"
      },
      "source": [
        "a = torch.normal(mean=torch.zeros(2,4))\n",
        "b = torch.normal(mean=torch.zeros(2, 4))\n",
        "print(f\"a:\\n{a}\\nb:\\n{b}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKUxAfveR6iQ"
      },
      "source": [
        "a = a.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klvLnHmaSDbk"
      },
      "source": [
        "a + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11l9FxZVSEDL"
      },
      "source": [
        "(a + b.cuda()).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFkGBACsSmL4"
      },
      "source": [
        "### Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr46uuNNSR_U"
      },
      "source": [
        "a = torch.randn(2, requires_grad=True)\n",
        "b = torch.normal(mean=torch.zeros(2))\n",
        "\n",
        "c = torch.dot(a, b)\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmiq2lLnSvO4"
      },
      "source": [
        "c.backward()\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\n(a,b): {c}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8I2iLEyS3dU"
      },
      "source": [
        "print(f\"Grad a: {a.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFD2nov7UDlf"
      },
      "source": [
        "Add function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2P8UECTnEO"
      },
      "source": [
        "a = torch.randn(2, requires_grad=True)\n",
        "b = torch.normal(mean=torch.zeros(2))\n",
        "c = torch.ones(1, requires_grad=True)\n",
        "\n",
        "d = torch.sigmoid(torch.dot(a, b) + c)\n",
        "print(f'a:\\n{a}\\nb:\\n{b}\\nSigmoid( (a,b) ): {d}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur_QA6xbTzeG"
      },
      "source": [
        "print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsheM7soT6sw"
      },
      "source": [
        "d.backward()\n",
        "print(f\"Grad a: {a.grad}\\nGrad c: {c.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf9pO1XJUFZI"
      },
      "source": [
        "Okay, what about vectors?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ginrHkBT_HC"
      },
      "source": [
        "a = torch.randn(2, requires_grad=True)\n",
        "b = torch.normal(mean=torch.zeros(2))\n",
        "\n",
        "c = a * b\n",
        "c.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sNYtrJ3YxV3"
      },
      "source": [
        "## Neural Network. Rewind"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MKsdG6GdorV"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "import mnist\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waQtAlplIh6N"
      },
      "source": [
        "images = mnist.train_images() / 255\n",
        "labels = mnist.train_labels()\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(images, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOHbffeqHyeF"
      },
      "source": [
        "def get_batches(dataset, batch_size):\n",
        "    X = dataset[0].reshape(-1, 28 * 28)\n",
        "    Y = dataset[1]\n",
        "    n_samples = X.shape[0]\n",
        "    \n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "    \n",
        "        yield torch.FloatTensor(X[batch_idx]), torch.LongTensor(Y[batch_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R402HnG8ULdl"
      },
      "source": [
        "class CustomLinear:\n",
        "    def __init__(self, in_size, out_size):\n",
        "        \"\"\"\n",
        "        Simple linear layer\n",
        "        \"\"\"\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        self.w = ...  # torch.randn\n",
        "        self.b = ...  # torch.randn\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return ...  # torch.mm\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.w.grad = None\n",
        "        self.b.grad = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzDzCXmtZivn"
      },
      "source": [
        "class CustomNeuralNetwork:\n",
        "    def __init__(self, dims, activation=\"sigmoid\"):\n",
        "        \"\"\"\n",
        "        Simple deep networks, that joins several linear layers. \n",
        "        \"\"\"\n",
        "        self.dims = dims\n",
        "\n",
        "        self.linears = ...  # list of linear layers\n",
        "        if activation == \"sigmoid\":\n",
        "            self.activation = torch.sigmoid\n",
        "        elif activation == \"relu\":\n",
        "            self.activation = torch.relu\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return ...\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for l in self.linears:\n",
        "            l.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZtMZC_dbmFS"
      },
      "source": [
        "class CustomCrossEntropy:\n",
        "    def __call__(self, x, target):\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t47gvIlUkFpT"
      },
      "source": [
        "def simple_sgd(model, config):\n",
        "    with torch.no_grad():\n",
        "        for l in model.linears:\n",
        "            l.w -= config['learning_rate'] * l.w.grad\n",
        "            l.b -= config['learning_rate'] * l.b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ER9Cs5jaoNi"
      },
      "source": [
        "net = CustomNeuralNetwork((28 * 28, 10, 10))\n",
        "criterion = CustomCrossEntropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOQeiTU0Jnkr"
      },
      "source": [
        "optimizer_config = {\n",
        "    \"learning_rate\": 1e-1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nwAo4jfaxry"
      },
      "source": [
        "def train(model, optimizer_config, n_epoch=20, batch_size=256):\n",
        "    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n",
        "    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "            model.zero_grad()\n",
        "            \n",
        "            predictions = model(x_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            simple_sgd(model, optimizer_config)      \n",
        "            \n",
        "            step += 1\n",
        "            train_logs[\"Train Loss\"].append(loss.detach().item())\n",
        "            train_logs[\"Steps\"].append(step)\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch, np.argmax(predictions.numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "        valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n",
        "        valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n",
        "        valid_logs[\"Steps\"].append(step)\n",
        "\n",
        "        if best_valid_loss > sum_loss / count_valid_steps:\n",
        "            best_valid_loss = sum_loss / count_valid_steps\n",
        "            best_model = deepcopy(model)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n",
        "    plt.plot()\n",
        "\n",
        "    return best_model, train_logs, valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufy_8neBa3hG"
      },
      "source": [
        "net, _, _ = train(net, optimizer_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnI3eIEeLZT"
      },
      "source": [
        "## Neural Network. Rewind #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLgLKCUfcCG3"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        self.w = ...  # nn.Parameter\n",
        "        self.b = ...  # nn.Parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...  # torch.mm\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self, in_size, alpha=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_size = in_size\n",
        "\n",
        "        self.beta = ...  # nn.Parameter\n",
        "        self.gamma = ...  # nn.Parameter\n",
        "\n",
        "        self.epsilon = 1e-5\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...\n",
        "\n",
        "\n",
        "class Dropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ...\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_size, out_size, activation=\"sigmoid\", p=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = Linear(in_size, out_size)\n",
        "        self.dropout = Dropout(p)\n",
        "        self.batch_norm = BatchNorm(out_size)\n",
        "        \n",
        "        if activation == \"sigmoid\":\n",
        "            self.activation = torch.sigmoid\n",
        "        elif activation == \"relu\":\n",
        "            self.activation = torch.relu\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = ...\n",
        "        x = ...\n",
        "        x = ...\n",
        "        return ...\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, dims, activation=\"relu\", p=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            list(Block(d_0, d_1, activation=activation, p=p) for d_0, d_1 in zip(dims[:-2], dims[1:-1]))\n",
        "        )\n",
        "        self.cl = Linear(dims[-2], dims[-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for m in self.blocks:\n",
        "            x = m(x)\n",
        "        return self.cl(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcqQ6AwPczjI"
      },
      "source": [
        "net = Net((28 * 28, 100, 10), p=0.1).cuda()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZtluygsczgm"
      },
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2iqWQojL3J7"
      },
      "source": [
        "def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n",
        "    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n",
        "    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = Nonein\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            predictions = model(x_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()   \n",
        "            \n",
        "            step += 1\n",
        "            train_logs[\"Train Loss\"].append(loss.detach().item())\n",
        "            train_logs[\"Steps\"].append(step)\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "            valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n",
        "            valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n",
        "            valid_logs[\"Steps\"].append(step)\n",
        "\n",
        "            if best_valid_loss > sum_loss / count_valid_steps:\n",
        "                best_valid_loss = sum_loss / count_valid_steps\n",
        "                best_model = deepcopy(net)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n",
        "    plt.plot()\n",
        "\n",
        "    return best_model, train_logs, valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkpB7TtL3IU"
      },
      "source": [
        "net, _, _ = train(net, optimizer, device=\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcOEa7lcjiyO"
      },
      "source": [
        "### Neural Network. Rewind #3. Logging\n",
        "\n",
        "Logging systems:\n",
        "- [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n",
        "- [WandB](https://www.wandb.com/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjRHFJXSjn81"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_size, out_size, activation=\"relu\", p=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_size = in_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        if activation == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid\n",
        "        elif activation == \"relu\":\n",
        "            self.activation = nn.ReLU\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            ...  # Linear, BatchNorm1d, Dropout, Activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "net = nn.Sequential(Block(28 * 28, 100, p=0.2), nn.Linear(100, 10)).cuda()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV1AxNFbfapY"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgo_htQZfamn"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWa59wKf5Jd"
      },
      "source": [
        "def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n",
        "    writer = SummaryWriter(Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            predictions = model(x_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer.step()   \n",
        "            \n",
        "            step += 1\n",
        "            writer.add_scalar(\"Train Loss\", loss.detach().item(), step)\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in get_batches((X_valid, y_valid), batch_size):\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "            writer.add_scalar(\"Valid Loss\", sum_loss / count_valid_steps, step)\n",
        "            writer.add_scalar(\"Valid Accuracy\", sum_acc / count_valid_steps, step)\n",
        "\n",
        "            if best_valid_loss > sum_loss / count_valid_steps:\n",
        "                best_valid_loss = sum_loss / count_valid_steps\n",
        "                best_model = deepcopy(net)\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qptL6E_Mf5G5"
      },
      "source": [
        "net = train(net, optimizer, device=\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuzkIqc0f5FG"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1zyKnXFdxt"
      },
      "source": [
        "## Spoiler - train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "- [A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide)\n",
        "- [Docs](https://catalyst-team.github.io/catalyst/)\n",
        "- [What is Runner?](https://catalyst-team.github.io/catalyst/api/core.html#runner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y_cQnMeJJiU"
      },
      "source": [
        "! pip install catalyst==21.04.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm2SIio7f5Cd"
      },
      "source": [
        "import typing as tp\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl, metrics, utils\n",
        "from catalyst.contrib.datasets import MNIST\n",
        "from catalyst.data.transforms import ToTensor\n",
        "\n",
        "model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loaders = {\n",
        "    \"train\": DataLoader(\n",
        "        MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32,\n",
        "    ),\n",
        "    \"valid\": DataLoader(\n",
        "        MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32,\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class CustomRunner(dl.Runner):\n",
        "    def predict_batch(self, batch: tp.Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "        # model inference step\n",
        "        return self.model(batch[0].to(self.device))\n",
        "\n",
        "    def on_loader_start(self, runner):\n",
        "        super().on_loader_start(runner)\n",
        "        self.meters = {\n",
        "            key: metrics.AdditiveValueMetric(compute_on_call=False)\n",
        "            for key in [\"loss\", \"accuracy01\", \"accuracy03\"]\n",
        "        }\n",
        "\n",
        "    def handle_batch(self, batch: tp.Tuple[torch.Tensor, torch.Tensor]):\n",
        "        # model train/valid step\n",
        "        # unpack the batch\n",
        "        x, y = batch\n",
        "        # run model forward pass\n",
        "        y_hat = self.model(x)\n",
        "        # compute the loss\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        # compute the metrics\n",
        "        accuracy01, accuracy03 = metrics.accuracy(y_hat, y, topk=(1, 3))\n",
        "        # log metrics\n",
        "        self.batch_metrics.update(\n",
        "            {\"loss\": loss, \"accuracy01\": accuracy01, \"accuracy03\": accuracy03}\n",
        "        )\n",
        "        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n",
        "            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n",
        "        # run model backward pass\n",
        "        if self.is_train_loader:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "    def on_loader_end(self, runner):\n",
        "        for key in [\"loss\", \"accuracy01\", \"accuracy03\"]:\n",
        "            self.loader_metrics[key] = self.meters[key].compute()[0]\n",
        "        super().on_loader_end(runner)\n",
        "\n",
        "\n",
        "runner = CustomRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    engine=dl.DeviceEngine(device=utils.get_device()),\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    criterion=criterion,\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    num_epochs=5,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        ")\n",
        "\n",
        "# model inference\n",
        "for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
        "    assert prediction.detach().cpu().numpy().shape[-1] == 10\n",
        "\n",
        "features_batch = next(iter(loaders[\"valid\"]))[0]\n",
        "# model tracing\n",
        "utils.trace_model(model=runner.model, batch=features_batch)\n",
        "# model quantization\n",
        "utils.quantize_model(model=runner.model)\n",
        "# model pruning\n",
        "utils.prune_model(model=runner.model, pruning_fn=\"l1_unstructured\", amount=0.8)\n",
        "# onnx export\n",
        "# utils.onnx_export(model=runner.model, batch=features_batch, file=\"./logs/mnist.onnx\", verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUz3o_kJJIHe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}