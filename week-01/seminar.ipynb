{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHEN61c1OwMv"
   },
   "source": [
    "# Seminar 1. Intro.\n",
    "Hi! Today we are going to recall our memories about numpy, sci-kit learn, matplotlib, make our own simple Neural Network and train it to solve some classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZR2Z-0tN0WG"
   },
   "outputs": [],
   "source": [
    "! pip install numpy scikit-learn matplotlib mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPUehjglN5Vn"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mf_26P2DOeWg"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XZHmVxNOsmM"
   },
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2rOjOL1EiB3"
   },
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "If5mxOaLOjBb"
   },
   "outputs": [],
   "source": [
    "a = [1. , 1.4 , 2.5]\n",
    "print(f\"Simple way: {np.array(a)}\")\n",
    "print(f\"Zeros:\\n {np.zeros((2,3))}\")\n",
    "print(f\"Range: {np.arange(10)}\")\n",
    "print(f\"Complicated range: {np.arange(4, 12, 2)}\")\n",
    "print(f\"Space: {np.linspace(1, 4, 6)}\")\n",
    "print(f\"Identity matrix:\\n {np.eye(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ComMD10nEkxk"
   },
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PYocIZHEmiu"
   },
   "outputs": [],
   "source": [
    "print(f\"From 0 to 1: {np.random.rand()}\")\n",
    "print(f\"Vector from 0 to 1: {np.random.rand(5)}\")\n",
    "print(f\"Vector from 0 to 10: {np.random.randint(10, size=5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDzRMMd_F3XW"
   },
   "source": [
    "### Matrix Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twvUXA9n_INY"
   },
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "b = np.linspace(-10, 10, 10)\n",
    "print(f\"a: {a}\\nshape:{a.shape}\")\n",
    "print(f\"b: {a}\\nshape:{b.shape}\")\n",
    "print(f\"a + b: {a + b},\\n\\t a * b: {a * b}\")\n",
    "print(f\"Dot product: {a.dot(b)}\")\n",
    "print(f\"Mean: {a.mean()}, STD: {a.std()}\")\n",
    "print(f\"Sum: {a.sum()}, Min: {a.min()}, Max: {a.max()}\")\n",
    "print(f\"Reshape:\\n{a.reshape(-1, 1)}\\nshape: {a.reshape(-1, 1).shape}\")\n",
    "c = a.reshape(-1, 1).repeat(5, axis=1)\n",
    "print(f\"Repeat:\\n{c}\")\n",
    "print(f\"Transpose:\\n{c.T}\\nshape: {c.T}\")\n",
    "print(f\"Unique items: {np.unique(c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4LmocC6F7Po"
   },
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDPlf50O_IK5"
   },
   "outputs": [],
   "source": [
    "a = np.arange(100).reshape(10, 10)\n",
    "print(f\"Array:\\n{a}\\nshape:{a.shape}\")\n",
    "print(f\"Get first column: {a[:, 0]}\")\n",
    "print(f\"Get last row: {a[-1, :]}\")\n",
    "print(f\"Add new awis:\\n{a[:, np.newaxis]}\\nshape: {a[:, np.newaxis].shape}\")\n",
    "print(f\"Specific indexing:\\n{a[4:6, 7:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6T3mE9ACPXSe"
   },
   "source": [
    "## Scikit Learn\n",
    "[Docs](https://scikit-learn.org/stable/modules/classes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvmLjGOlPYvJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mmGgreBPZYU"
   },
   "source": [
    "## Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzMTqUUZPbT9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chDEutqxH7Tj"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10), 6 * np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UZ_HAzbH7SN"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs((50, 50, 50))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1qT5V355H7Po"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Adversal data\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5Y4ULaGH7NZ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=1.0)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=.3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YGPOoKnPkXh"
   },
   "source": [
    "## Neural Network\n",
    "Based on [ml-mipt](https://github.com/girafe-ai/ml-mipt) course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AoBqI7MUPmHB"
   },
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "\n",
    "\n",
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eE6e6flB6FNq"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially. \n",
    "         \n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "            \n",
    "        Just write a little loop. \n",
    "        \"\"\"\n",
    "\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = input\n",
    "        \n",
    "        for module in self.modules:\n",
    "            self.output = module.forward(self.output)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "             \n",
    "             \n",
    "        !!!\n",
    "                \n",
    "        To ech module you need to provide the input, module saw while forward pass, \n",
    "        it is used while computing gradients. \n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
    "        and NOT `input` to this Sequential module. \n",
    "        \n",
    "        !!!\n",
    "        \n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        for i in range(len(self.modules)-1, 0, -1):\n",
    "            gradOutput = self.modules[i].backward(self.modules[i-1].output, gradOutput)\n",
    "        \n",
    "        self.gradInput = self.modules[0].backward(input, gradOutput)\n",
    "        \n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "efNLSSFS6FFc"
   },
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5AH9tM962yD"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D _input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, _input):\n",
    "        # Your code goes here. ################################################\n",
    "        self.output = np.dot(_input, self.W.T) + self.b\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, _input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        self.gradInput = np.dot(gradOutput, self.W)\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, _input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        # self.gradW = ... ; self.gradb = ...\n",
    "        self.gradW += np.dot(gradOutput.T, _input)\n",
    "        self.gradb += np.sum(gradOutput, axis=0)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICQHX5t_6-o7"
   },
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, _input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(_input, _input.max(axis=1, keepdims=True))\n",
    "        \n",
    "        # Your code goes here. ################################################\n",
    "        self.output = - _input - np.log(np.sum(np.exp(-_input), axis=1)).reshape(-1, 1)\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, _input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        _input = np.subtract(_input, _input.max(axis=1, keepdims=True))\n",
    "        s = 1 / np.sum(np.exp(-_input), axis=1).reshape(-1, 1)\n",
    "        self.gradInput = - gradOutput + gradOutput * s * np.exp(- _input)\n",
    "\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2BolEaJ7B9v"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, _input):\n",
    "        self.output = np.maximum(_input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, _input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , _input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jveUm1Ta7Fa2"
   },
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, _input, target): \n",
    "        # Your code goes here. ################################################\n",
    "\n",
    "        self.output = - np.mean(_input[np.arange(target.shape[0]), target])\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, _input, target):\n",
    "        # Your code goes here. ################################################\n",
    "\n",
    "        self.gradInput = np.zeros_like(_input)\n",
    "        self.gradInput[np.arange(target.shape[0]), target] = -1 / _input.shape[0]\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbe8ik157T3D"
   },
   "outputs": [],
   "source": [
    "def simple_sgd(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('accumulated_grads', {})\n",
    "    \n",
    "    var_index = 0 \n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            \n",
    "            current_var -= config['learning_rate'] * current_grad\n",
    "            var_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SiNkcvbPnNX"
   },
   "source": [
    "### Simple classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHpD7FvlPqQZ"
   },
   "outputs": [],
   "source": [
    "dataset = make_blobs((200, 200, 200))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[0], dataset[1], test_size=0.3)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=1.0)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=.3)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2i4WBvahK7Ov"
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 3))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guAcoLa2K7Mo"
   },
   "outputs": [],
   "source": [
    "# Optimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-2}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xQAbg-ZK7KU"
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_idx = indices[start:end]\n",
    "    \n",
    "        yield X[batch_idx], Y[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnVR-lt_Vp3x"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss_history = []\n",
    "    val_loss_history = [0,]\n",
    "    val_acc_history = [0,]\n",
    "    steps = [0,]\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        steps.append(steps[-1])\n",
    "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
    "            \n",
    "            net.zeroGradParameters()\n",
    "            \n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "        \n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            net.backward(x_batch, dp)\n",
    "            \n",
    "            # Update weights\n",
    "            simple_sgd(net.getParameters(), \n",
    "                    net.getGradParameters(), \n",
    "                    optimizer_config,\n",
    "                    optimizer_state)      \n",
    "            \n",
    "            train_loss_history.append(loss)\n",
    "            steps[-1] += 1\n",
    "\n",
    "        sum_loss = 0\n",
    "        sum_acc = 0\n",
    "        count_val_steps = 0\n",
    "        for x_batch, y_batch in get_batches((X_val, y_val), batch_size):\n",
    "            predictions = net.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "            sum_loss += loss\n",
    "            sum_acc += accuracy_score(y_batch, np.argmax(predictions, axis=1))\n",
    "            count_val_steps += 1\n",
    "\n",
    "        val_loss_history.append(sum_loss / count_val_steps)\n",
    "        val_acc_history.append(sum_acc / count_val_steps)\n",
    "\n",
    "        # Visualize\n",
    "        display.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "            \n",
    "        ax[0].set_title(\"Training loss\")\n",
    "        ax[0].set_xlabel(\"#iteration\")\n",
    "        ax[0].set_ylabel(\"loss\")\n",
    "        ax[0].plot(train_loss_history, 'b')\n",
    "        ax[0].grid()\n",
    "\n",
    "        ax[1].set_title(\"Validation loss\")\n",
    "        ax[1].set_xlabel(\"#iteration\")\n",
    "        ax[1].set_ylabel(\"loss\")\n",
    "        ax[1].plot(steps, val_loss_history, 'b')\n",
    "        ax[1].grid()\n",
    "\n",
    "        ax[2].set_title(\"Validation Accuracy\")\n",
    "        ax[2].set_xlabel(\"#iteration\")\n",
    "        ax[2].set_ylabel(\"accuracy\")\n",
    "        ax[2].plot(steps, val_acc_history, 'b')\n",
    "        ax[2].grid()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print('Current loss: %f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVh-Kla5n2Va"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eunZzRfZnXyC"
   },
   "source": [
    "Let's make network more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8PR9W8UVpwK"
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 10))\n",
    "net.add(ReLU())\n",
    "net.add(Linear(10, 3))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgEOKcMQK7Hm"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SLowBWmPq74"
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uu9n-r20Pr1b"
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "\n",
    "\n",
    "images = mnist.train_images() / 255\n",
    "labels = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zze-lKruoi3w"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax[i // 5, i % 5].imshow(images[i], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0C-svqMdo0ld"
   },
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZdRaWWIphZe"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "new_features = pca.fit_transform([i.reshape(-1) for i in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEyM9qzMpXUZ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(new_features[:, 0], new_features[:, 1], c=labels)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwdeElhWqJva"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAjrtSersbOC"
   },
   "outputs": [],
   "source": [
    "# Optimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-2}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qczTQ6oqpyEu"
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(28*28, 10))\n",
    "net.add(LogSoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRAGNc6nqCVh"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss_history = []\n",
    "    val_loss_history = [0,]\n",
    "    val_acc_history = [0,]\n",
    "    steps = [0,]\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        steps.append(steps[-1])\n",
    "        for x_batch, y_batch in get_batches((X_train, y_train), batch_size):\n",
    "            \n",
    "            net.zeroGradParameters()\n",
    "            x_batch = x_batch.reshape(-1, 28*28)\n",
    "            \n",
    "            # Forward\n",
    "            predictions = net.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "        \n",
    "            # Backward\n",
    "            dp = criterion.backward(predictions, y_batch)\n",
    "            net.backward(x_batch, dp)\n",
    "            \n",
    "            # Update weights\n",
    "            simple_sgd(net.getParameters(), \n",
    "                    net.getGradParameters(), \n",
    "                    optimizer_config,\n",
    "                    optimizer_state)      \n",
    "            \n",
    "            train_loss_history.append(loss)\n",
    "            steps[-1] += 1\n",
    "\n",
    "        sum_loss = 0\n",
    "        sum_acc = 0\n",
    "        count_val_steps = 0\n",
    "        for x_batch, y_batch in get_batches((X_val, y_val), batch_size):\n",
    "            x_batch = x_batch.reshape(-1, 28*28)\n",
    "            predictions = net.forward(x_batch)\n",
    "            loss = criterion.forward(predictions, y_batch)\n",
    "            sum_loss += loss\n",
    "            sum_acc += accuracy_score(y_batch, np.argmax(predictions, axis=1))\n",
    "            count_val_steps += 1\n",
    "\n",
    "        val_loss_history.append(sum_loss / count_val_steps)\n",
    "        val_acc_history.append(sum_acc / count_val_steps)\n",
    "\n",
    "        # Visualize\n",
    "        display.clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "        ax[0].set_title(\"Training loss\")\n",
    "        ax[0].set_xlabel(\"#iteration\")\n",
    "        ax[0].set_ylabel(\"loss\")\n",
    "        ax[0].plot(train_loss_history, 'b')\n",
    "        ax[0].grid()\n",
    "\n",
    "        ax[1].set_title(\"Validation loss\")\n",
    "        ax[1].set_xlabel(\"#iteration\")\n",
    "        ax[1].set_ylabel(\"loss\")\n",
    "        ax[1].plot(steps, val_loss_history, 'b')\n",
    "        ax[1].grid()\n",
    "\n",
    "        ax[2].set_title(\"Validation Accuracy\")\n",
    "        ax[2].set_xlabel(\"#iteration\")\n",
    "        ax[2].set_ylabel(\"accuracy\")\n",
    "        ax[2].plot(steps, val_acc_history, 'b')\n",
    "        ax[2].grid()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print('Current loss: %f' % (sum_loss / count_val_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SrSQjT9qZA1"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVblbAuIqaQL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1 Seminar",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:py37-dev]",
   "language": "python",
   "name": "conda-env-py37-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
